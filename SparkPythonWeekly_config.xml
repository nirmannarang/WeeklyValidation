<?xml version='1.0' encoding='UTF-8'?>
<project>
  <actions/>
  <description></description>
  <keepDependencies>false</keepDependencies>
  <properties/>
  <scm class="hudson.scm.NullSCM"/>
  <canRoam>true</canRoam>
  <disabled>false</disabled>
  <blockBuildWhenDownstreamBuilding>false</blockBuildWhenDownstreamBuilding>
  <blockBuildWhenUpstreamBuilding>false</blockBuildWhenUpstreamBuilding>
  <triggers>
    <jenkins.triggers.ReverseBuildTrigger>
      <spec></spec>
      <upstreamProjects>10.77.67.159-SparkBranch-2.0-Hadoop-2.7-without-Hive-OPENJDK-WeeklyBuild</upstreamProjects>
      <threshold>
        <name>SUCCESS</name>
        <ordinal>0</ordinal>
        <color>BLUE</color>
        <completeBuild>true</completeBuild>
      </threshold>
    </jenkins.triggers.ReverseBuildTrigger>
  </triggers>
  <concurrentBuild>false</concurrentBuild>
  <builders>
    <hudson.tasks.Shell>
      <command>#!/bin/bash

echo &quot;Started Spark-PythonUnitTest&quot;

workDir=/root/WeeklyValidation
IPbaremetal=10.77.67.159
userName=$(grep -Po &apos;(?&lt;=userName=).*&apos; ${workDir}/baremetalMachines/${IPbaremetal})
passWord=$(grep -Po &apos;(?&lt;=passWord=).*&apos; ${workDir}/baremetalMachines/${IPbaremetal})

ssh ${userName}@${IPbaremetal} /bin/bash &lt;&lt;&apos;EOF&apos;
echo &quot;These commands will be run on: $( uname -a )&quot;
echo &quot;They are executed by: $( whoami )&quot;
cd WeeklyValidation
workDirR=$(pwd)
export SNAPPY_HOME=/usr/lib
export LEVELDB_HOME=${workDirR}/leveldb
export LEVELDBJNI_HOME=${workDirR}/leveldbjni
export LIBRARY_PATH=${SNAPPY_HOME}
export C_INCLUDE_PATH=${LIBRARY_PATH}
export CPLUS_INCLUDE_PATH=${LIBRARY_PATH}
IPbaremetalR=10.77.67.159
cd baremetalMachines/
FunctionalTests=$(grep -Po &apos;(?&lt;=FunctionalTests=).*&apos; ${IPbaremetalR})
PythonTests=$(grep -Po &apos;(?&lt;=PythonTests=).*&apos; ${IPbaremetalR})
RTests=$(grep -Po &apos;(?&lt;=RTests=).*&apos; ${IPbaremetalR})
jdk_val=$(grep -Po &apos;(?&lt;=JDK_VAL=).*&apos; ${IPbaremetalR})
branchClone=$(grep -Po &apos;(?&lt;=branchClone=).*&apos; ${IPbaremetalR})
hiveBuild=$(grep -Po &apos;(?&lt;=buildWithHive=).*&apos; ${IPbaremetalR})
hadoopVer=$(grep -Po &apos;(?&lt;=hadoopVer=).*&apos; ${IPbaremetalR})

cd ${workDirR}

if [ $hiveBuild == TRUE ]
then
  hiveFlag=with
elif [ $hiveBuild == FALSE ]
then
  hiveFlag=without
fi


echo -en &apos;#Creating workspace directories for jobs\n&apos;
mkdir -p ${workDirR}/workspace/${IPbaremetalR}-SparkBranch-${branchClone}-Hadoop-${hadoopVer}-${hiveFlag}-Hive-${jdk_val}-WeeklyPythonTests
cd ${workDirR}/workspace/${IPbaremetalR}-SparkBranch-${branchClone}-Hadoop-${hadoopVer}-${hiveFlag}-Hive-${jdk_val}-WeeklyBuild/spark

if [ ${jdk_val} = &quot;OPENJDK&quot; ]
then
  if [ &quot;$(. /etc/os-release; echo $NAME)&quot; = &quot;Ubuntu&quot; ]; then
        echo -en &quot;Setting OpenJDK path and JAVA_HOME\n&quot;
        export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-ppc64el
        export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH
      else
        echo -en &quot;Setting OpenJDK path and JAVA_HOME\n&quot;
        export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk
        export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH
      fi
elif [ ${jdk_val} = &quot;IBMJDK&quot; ]
then
  export JAVA_HOME=$(grep -Po &apos;(?&lt;=USER_INSTALL_DIR=).*&apos; ${workDirR}/baremetalMachines/${IPbaremetalR})
  export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH
fi

java -version

# Build Spark
# This enables yarn and hadoop profiles.
# We do not specify a yarn.version and assume it is same as hadoop.version
#
# A hadoop.version must be compatible with the hadoop profile. For this
# reason, we only build against version 2.6.0.
#

python/run-tests
#R/run-tests.sh

EOF

now=$(date +&quot;%d-%m-%Y_%H:%M:%S&quot;)

cp /var/lib/jenkins/jobs/${JOB_NAME}/builds/${BUILD_NUMBER}/log ${workDir}/logs/${JOB_NAME}/${JOB_NAME}_${now}.log
</command>
    </hudson.tasks.Shell>
  </builders>
  <publishers/>
  <buildWrappers/>
</project>
