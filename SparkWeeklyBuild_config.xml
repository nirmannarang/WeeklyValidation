<?xml version='1.0' encoding='UTF-8'?>
<project>
  <actions/>
  <description></description>
  <keepDependencies>false</keepDependencies>
  <properties/>
  <scm class="hudson.scm.NullSCM"/>
  <canRoam>true</canRoam>
  <disabled>false</disabled>
  <blockBuildWhenDownstreamBuilding>false</blockBuildWhenDownstreamBuilding>
  <blockBuildWhenUpstreamBuilding>false</blockBuildWhenUpstreamBuilding>
  <triggers>
    <hudson.triggers.TimerTrigger>
      <spec>0 8 * * 1
</spec>
    </hudson.triggers.TimerTrigger>
  </triggers>
  <concurrentBuild>false</concurrentBuild>
  <builders>
    <hudson.tasks.Shell>
      <command>#!/bin/bash


workDir=/root/WeeklyValidation
IPbaremetal=10.77.67.159
#sudo scp -r ${workDir} root@${IPbaremetal}:/root/sshTest
ssh root@${IPbaremetal} /bin/bash &lt;&lt;&apos;EOF&apos;
echo &quot;These commands will be run on: $( uname -a )&quot;
echo &quot;They are executed by: $( whoami )&quot;
cd /root/sshTest/WeeklyValidation
workDirR=$(pwd)
IPbaremetalR=10.77.67.159
cd baremetalMachines/
FunctionalTests=$(grep -Po &apos;(?&lt;=FunctionalTests=).*&apos; ${IPbaremetalR})
PythonTests=$(grep -Po &apos;(?&lt;=PythonTests=).*&apos; ${IPbaremetalR})
RTests=$(grep -Po &apos;(?&lt;=RTests=).*&apos; ${IPbaremetalR})
jdk_val=$(grep -Po &apos;(?&lt;=JDK_VAL=).*&apos; ${IPbaremetalR})
branchClone=$(grep -Po &apos;(?&lt;=branchClone=).*&apos; ${IPbaremetalR})
hiveBuild=$(grep -Po &apos;(?&lt;=buildWithHive=).*&apos; ${IPbaremetalR})
hadoopVer=$(grep -Po &apos;(?&lt;=hadoopVer=).*&apos; ${IPbaremetalR})

cd ${workDirR}

if [ $hiveBuild == TRUE ]
then
  hiveFlag=with
elif [ $hiveBuild == FALSE ]
then
  hiveFlag=without
fi


echo -en &apos;#Creating workspace directories for jobs\n&apos;
mkdir -p ${workDirR}/workspace/${IPbaremetalR}-SparkBranch-${branchClone}-Hadoop-${hadoopVer}-${hiveFlag}-Hive-${jdk_val}-WeeklyBuild
cd ${workDirR}/workspace/${IPbaremetalR}-SparkBranch-${branchClone}-Hadoop-${hadoopVer}-${hiveFlag}-Hive-${jdk_val}-WeeklyBuild

rm -rf spark

git clone --recursive https://github.com/apache/spark.git -b branch-${branchClone}

cd spark

if [ $jdk_val == OPENJDK ]
then
  export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-ppc64el
  export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH
elif [ $jdk_val == IBMJDK ]
then
  export JAVA_HOME=$(grep -Po &apos;(?&lt;=USER_INSTALL_DIR=).*&apos; ${workDirR}/baremetalMachines/${IPbaremetalR})
  export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH
fi

java -version

echo $hiveBuild

if [ $hiveBuild == TRUE ]
then
  echo &quot; Building with Hive and JDBC Support \n &quot;
  build/mvn -Pyarn -Phadoop-${hadoopVer} -Psparkr -Dhadoop.version=${hadoopVer}.0 -Phive -Phive-thriftserver -DskipTests clean package
elif [ $hiveBuild == FALSE ]
then
  echo &quot; Building without Hive and JDBC Support \n &quot;
  build/mvn -Pyarn -Phadoop-${hadoopVer} -Psparkr -Dhadoop.version=${hadoopVer}.0 -DskipTests clean package
fi

EOF
 


#export &apos;_JAVA_OPTIONS=-XX:-UseGCOverheadLimit -Xms512m -Xmx2048m&apos;

#echo &quot;==============Clone Spark existing release======================&quot;


# Build Spark
# This enables yarn and hadoop profiles.
# We do not specify a yarn.version and assume it is same as hadoop.version
#
# A hadoop.version must be compatible with the hadoop profile. For this
# reason, we only build against version 2.6.0.
#


# Run tests
#build/mvn --fail-never -Pyarn -Phadoop-2.7 -Dhadoop.version=2.7.0 test
#python/run-tests
#R/run-tests.sh

#Backup log with timestamp
now=$(date +&quot;%d-%m-%Y_%H:%M:%S&quot;)

sudo cp /var/lib/jenkins/jobs/${JOB_NAME}/builds/${BUILD_NUMBER}/log ${workDir}/logs/${JOB_NAME}/${JOB_NAME}_${now}.log
      </command>
    </hudson.tasks.Shell>
  </builders>
  <publishers/>
  <buildWrappers/>
</project>