<?xml version='1.0' encoding='UTF-8'?>
<project>
  <actions/>
  <description></description>
  <keepDependencies>false</keepDependencies>
  <properties/>
  <scm class="hudson.scm.NullSCM"/>
  <canRoam>true</canRoam>
  <disabled>false</disabled>
  <blockBuildWhenDownstreamBuilding>false</blockBuildWhenDownstreamBuilding>
  <blockBuildWhenUpstreamBuilding>false</blockBuildWhenUpstreamBuilding>
  <triggers>
    <hudson.triggers.TimerTrigger>
      <spec>0 8 * * 1
</spec>
    </hudson.triggers.TimerTrigger>
  </triggers>
  <concurrentBuild>false</concurrentBuild>
  <builders>
    <hudson.tasks.Shell>
      <command>#!/bin/bash


workDir=/root/WeeklyValidation
IPbaremetal=10.77.67.159
userName=$(grep -Po &apos;(?&lt;=userName=).*&apos; ${workDir}/baremetalMachines/${IPbaremetal})
passWord=$(grep -Po &apos;(?&lt;=passWord=).*&apos; ${workDir}/baremetalMachines/${IPbaremetal})

ssh ${userName}@${IPbaremetal} /bin/bash &lt;&lt;&apos;EOF&apos;
echo &quot;These commands will be run on: $( uname -a )&quot;
echo &quot;They are executed by: $( whoami )&quot;
cd WeeklyValidation
workDirR=$(pwd)
export SNAPPY_HOME=/usr/lib
export LEVELDB_HOME=${workDirR}/leveldb
export LEVELDBJNI_HOME=${workDirR}/leveldbjni
export LIBRARY_PATH=${SNAPPY_HOME}
export C_INCLUDE_PATH=${LIBRARY_PATH}
export CPLUS_INCLUDE_PATH=${LIBRARY_PATH}
IPbaremetalR=10.77.67.159
cd baremetalMachines/
FunctionalTests=$(grep -Po &apos;(?&lt;=FunctionalTests=).*&apos; ${IPbaremetalR})
PythonTests=$(grep -Po &apos;(?&lt;=PythonTests=).*&apos; ${IPbaremetalR})
RTests=$(grep -Po &apos;(?&lt;=RTests=).*&apos; ${IPbaremetalR})
jdk_val=$(grep -Po &apos;(?&lt;=JDK_VAL=).*&apos; ${IPbaremetalR})
branchClone=$(grep -Po &apos;(?&lt;=branchClone=).*&apos; ${IPbaremetalR})
hiveBuild=$(grep -Po &apos;(?&lt;=buildWithHive=).*&apos; ${IPbaremetalR})
hadoopVer=$(grep -Po &apos;(?&lt;=hadoopVer=).*&apos; ${IPbaremetalR})

cd ${workDirR}

if [ $hiveBuild == TRUE ]
then
  hiveFlag=with
elif [ $hiveBuild == FALSE ]
then
  hiveFlag=without
fi


echo -en &apos;#Creating workspace directories for jobs\n&apos;
mkdir -p ${workDirR}/workspace/${IPbaremetalR}-SparkBranch-${branchClone}-Hadoop-${hadoopVer}-${hiveFlag}-Hive-${jdk_val}-WeeklyBuild
cd ${workDirR}/workspace/${IPbaremetalR}-SparkBranch-${branchClone}-Hadoop-${hadoopVer}-${hiveFlag}-Hive-${jdk_val}-WeeklyBuild

rm -rf spark

git clone --recursive https://github.com/apache/spark.git -b branch-${branchClone}

cd spark

if [ ${jdk_val} = &quot;OPENJDK&quot; ]
then
  if [ &quot;$(. /etc/os-release; echo $NAME)&quot; = &quot;Ubuntu&quot; ]; then
        echo -en &quot;Setting OpenJDK path and JAVA_HOME\n&quot;
        export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-ppc64el
        export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH
      else
        echo -en &quot;Setting OpenJDK path and JAVA_HOME\n&quot;
        export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk
        export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH
      fi
elif [ ${jdk_val} = &quot;IBMJDK&quot; ]
then
  export JAVA_HOME=$(grep -Po &apos;(?&lt;=USER_INSTALL_DIR=).*&apos; ${workDirR}/installer.properties)
  export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH
fi

java -version

echo $hiveBuild

if [ $hiveBuild == TRUE ]
then
  echo &quot; Building with Hive and JDBC Support \n &quot;
  #build/mvn -Pyarn -Phadoop-${hadoopVer} -Psparkr -Dhadoop.version=${hadoopVer}.0 -Phive -Phive-thriftserver -DskipTests clean package
  ./dev/make-distribution.sh --name custom-${branchClone}-spark --tgz -Psparkr -Phadoop-${hadoopVer} -Phive -Phive-thriftserver -Pyarn
elif [ $hiveBuild == FALSE ]
then
  echo &quot; Building without Hive and JDBC Support \n &quot;
  #build/mvn -Pyarn -Phadoop-${hadoopVer} -Psparkr -Dhadoop.version=${hadoopVer}.0 -DskipTests clean package
  ./dev/make-distribution.sh --name custom-${branchClone}-spark --tgz -Psparkr -Phadoop-${hadoopVer} -Pyarn
fi

EOF
 


#export &apos;_JAVA_OPTIONS=-XX:-UseGCOverheadLimit -Xms512m -Xmx2048m&apos;

#echo &quot;==============Clone Spark existing release======================&quot;


# Build Spark
# This enables yarn and hadoop profiles.
# We do not specify a yarn.version and assume it is same as hadoop.version
#
# A hadoop.version must be compatible with the hadoop profile. For this
# reason, we only build against version 2.6.0.
#


# Run tests
#build/mvn --fail-never -Pyarn -Phadoop-2.7 -Dhadoop.version=2.7.0 test
#python/run-tests
#R/run-tests.sh

#Backup log with timestamp
now=$(date +&quot;%d-%m-%Y_%H:%M:%S&quot;)

cp /var/lib/jenkins/jobs/${JOB_NAME}/builds/${BUILD_NUMBER}/log ${workDir}/logs/${JOB_NAME}/${JOB_NAME}_${now}.log
      </command>
    </hudson.tasks.Shell>
  </builders>
  <publishers/>
  <buildWrappers/>
</project>
